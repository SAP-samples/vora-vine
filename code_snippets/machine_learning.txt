----------QUERY1----------
CREATE VIEW KMEANS_INPUT_REFERENCE_DATA AS
SELECT PARTNERID, SUM(GROSSAMOUNT) as TOTAL_SALESAMOUNT, 
SUM(NOOFITEMS) as NOOFITEMS, CASE REGION WHEN 'APJ' THEN 1 WHEN 'AFR' THEN 2 WHEN 'AMER' THEN 3 WHEN 'EMEA' THEN 4 END as REGION
FROM ORDER_DETAILS
GROUP BY PARTNERID, REGION 
ORDER BY PARTNERID
USING com.sap.spark.view

----------QUERY2----------
SELECT * FROM  KMEANS_INPUT_REFERENCE_DATA

// Program 1
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.sql.functions._

//Load input data
val data = sc.textFile("/user/vora/kmeans_input_data.csv")

//Removing the headers
val header = data.first
val rows = data.filter(l => l != header)

//Defining the case class
case class caseClass(ID: String, TOTAL_SALESAMOUNT: Double, NOOFITEMS: Double, REGION: Int)

//Split each row by comma to get the columns
val rowSplit = rows.map(line => line.split(","))

//Mapping the data to case class
val rowData = rowSplit.map( p => caseClass( p(0).toString, p(1).trim.toDouble, p(2).trim.toDouble, p(3).trim.toInt))

//Creating dataframe from the rowData
val df1 = rowData.toDF()

//Convert to RDD with the rowId
val rdd1 = df1.rdd.map(r => (r.getString(0),  r.getDouble(1), r.getDouble(2), r.getInt(3)))

//Cache the RDD
rdd1.cache()

//Convert to RDD without rowId. This will be the input to the KMeans algorithm
val vectors = df1.rdd.map(r => Vectors.dense( r.getDouble(1), r.getDouble(2), r.getInt(3)))
//Cache the RDD
vectors.cache()

//KMeans model with 10 clusters and 30 iterations
val numClusters = 10
val numIterations = 30
val kMeansModel = KMeans.train(vectors, numClusters, numIterations)

//Print the center of each cluster
kMeansModel.clusterCenters.foreach(println)

//Get the clusters from the model and link it to the rowId using rdd1
val cluster = rdd1.map{r => (r._1, kMeansModel.predict(Vectors.dense(r._2, r._3, r._4) ))}

//Convert rdd to dataframe with rowId and cluster
val clusterDF = cluster.toDF("ID", "CLUSTER")

//Create temporary table from the df and display the results 
clusterDF.registerTempTable("CLUSTER_MAPPING")
clusterDF.show()

----------QUERY3----------
SELECT * FROM CLUSTER_MAPPING


// Program 2
import org.apache.spark.sql.SQLContext
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.fpm.FPGrowth
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.functions.collect_list
import org.apache.spark.sql.Row

//Loading data
var soItem = sqlContext.read.load("/user/vora/soItemData.parquet");
val texts = sqlContext.read.load("/user/vora/texts.parquet")
val products  = sqlContext.read.load("/user/vora/products.parquet")

// Grouping product based on salesOrder
soItem.registerTempTable("SO_ITEM");
var soItemView = sqlContext.sql("SELECT SALESORDERID, PRODUCTID from SO_ITEM")
soItemView.registerTempTable("SO_ITEM_VIEW");
var groupedData = sqlContext.sql("SELECT SALESORDERID, collect_set(PRODUCTID) as itemList from SO_ITEM_VIEW group by SALESORDERID")
groupedData.registerTempTable("GRP_BY_SOID")
var itemsPerTransaction = sqlContext.sql("SELECT itemList from GRP_BY_SOID")

// Converting List[String] to String
val mkString = udf((items:Seq[String])=>items.mkString(","))
val dataframeWithString = itemsPerTransaction.select($"itemList").withColumn("arrayString",mkString($"itemList"))
var dfwsNew = dataframeWithString.drop(dataframeWithString.col("itemList"))
var fpgInputData =  dfwsNew.rdd.map{x:Row => x.getAs[String](0)}

// Running FPGrowth Algorithm
val data: RDD[Array[String]] = fpgInputData.map(s => s.trim.split(','))
val fpGrowth = new FPGrowth().setMinSupport(0.005).setNumPartitions(10)
val fpGrowthModel = fpGrowth.run(data)

var freqItemset = (fpGrowthModel.freqItemsets.map{itemset => (itemset.items,  itemset.freq)})
var freqItemsetDF = freqItemset.toDF("frequentItems","frequency")
val freqItemCollection = freqItemsetDF.filter(size('frequentItems) > 1)


// replacing ProductID with product description
val freqItems = freqItemsetDF.filter(size('frequentItems) === 1).select('frequentItems(0).as('item), 'frequency)
texts.registerTempTable("TEXT")
products.registerTempTable("PRODUCTS")
val productDict = sqlContext.sql("SELECT PRODUCTID, SHORT_TEXT from PRODUCTS join TEXT where NAMEID == TEXTID")
val freqDict = freqItems.join(productDict, 'item === 'PRODUCTID, "left").select('item, 'SHORT_TEXT).as[(String, String)].collect().toMap

val pidToDescMapping = udf { items: Seq[String] => items.map(item => freqDict(item)) }
val freqItemDescs = freqItemCollection.select('frequency, pidToDescMapping('frequentItems).as('itemDescs))
val freqItemDescsString = freqItemDescs.select($"frequency",$"itemDescs").withColumn("ItemSet",mkString($"itemDescs"))
val frequentItemDesc = freqItemDescsString.select($"ItemSet",$"frequency")

// Normalising the frequency
val min = frequentItemDesc.groupBy().min().head().getLong(0)
val max = frequentItemDesc.groupBy().max().head().getLong(0)
val minInDouble = min.toDouble
val maxInDouble = max.toDouble
val normaliseUDF: (Long => Double) = (arg: Long) => {(arg - minInDouble)/(maxInDouble - minInDouble)}
val normalisedFreq = udf(normaliseUDF)
val frequentItems = frequentItemDesc.withColumn("normalised_frequency", round(normalisedFreq(col("frequency")),2))
frequentItems.registerTempTable("FREQUENT_ITEMS")

// Generating rules for frequent Items
val minConfidence = 0.8

var rules = (fpGrowthModel.generateAssociationRules(minConfidence).map{ rule => (rule.antecedent.map(item => freqDict(item)).mkString(","), rule.consequent.map(item => freqDict(item)).mkString(","))})
var rulesDF = rules.toDF("antecedent","consequent")
rulesDF.registerTempTable("RULES")


----------QUERY4----------
SELECT * FROM FREQUENT_ITEMS

----------QUERY5----------
SELECT * FROM RULES

----------QUERY6----------
SELECT consequent FROM RULES WHERE antecedent = "Printer AllRound,Laser STAR"  
